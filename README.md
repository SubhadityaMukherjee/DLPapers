# DEEP LEARNING PAPERS

- This repository contains implementations of various Deep learning papers
- Models have been trained for a little time and will give better results if trained more. The focus was more on understanding rather than results
- An index of papers implemented along with the citations and URLs are as follows
> Note: Codes in Pytorch

## INDEX

### MathBehind folder
- This is an attempt to codify and understand the math behind Deep learning. Inspired from the Deep Learning book (Ian Goodfellow et al.)
- All the explanations are in Jupyter Notebooks. (No README)
  - **[1]** Karush–Kuhn–Tucker

### Main
**[1]** Alex Net
- Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "Imagenet classification with deep convolutional neural networks." Advances in neural information processing systems. 2012
[Paper](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)

**[2]** VGG Net
 - Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014).
 [Paper](https://arxiv.org/pdf/1409.1556.pdf)

**[3]** GoogLe Net
 - Szegedy, Christian, et al. "Going deeper with convolutions." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015.
 [Paper](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf)

**[4]** Dropout (Just notes)
- Srivastava, Nitish, et al. "Dropout: a simple way to prevent neural networks from overfitting." Journal of Machine Learning Research 15.1 (2014): 1929-1958.
[Paper](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)

**[5]** Mobile Net
- MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications (2017), Andrew G. Howard et al.
[Paper](https://arxiv.org/pdf/1704.04861.pdf)

**[6]** Inceptionism
- Google Deep Dream [Link](https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html)

**[7]** DC GAN
- Radford, A., Metz, L., & Chintala, S. (2015). Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434.
[Paper](https://arxiv.org/pdf/1511.06434.pdf%C3)

**[8]** Spatial Transformer Networks
- Jaderberg, M., Simonyan, K., & Zisserman, A. (2015). Spatial transformer networks. In Advances in neural information processing systems (pp. 2017-2025).
[Paper](http://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf)

**[9]** Squeeze Net
- SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 1MB model size (2016), F. Iandola et al.
[Paper](http://arxiv.org/pdf/1602.07360)

**[10]** VAE (Auto-Encoding Variational Bayes)
- Kingma, D. P., & Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.
[Paper](https://arxiv.org/pdf/1312.6114.pdf?source=post_page---------------------------)

**[11]** SRCNN
- Dong, C., Loy, C. C., He, K., & Tang, X. (2014, September). Learning a deep convolutional network for image super-resolution. In European conference on computer vision (pp. 184-199). Springer, Cham.
[Paper](https://www.researchgate.net/profile/Chen_Change_Loy/publication/264552416_Lecture_Notes_in_Computer_Science/links/53e583e50cf25d674e9c280e.pdf)

**[12]** WGAN
- Arjovsky, M., Chintala, S., & Bottou, L. (2017). Wasserstein gan. arXiv preprint arXiv:1701.07875.
[Paper](https://arxiv.org/pdf/1701.07875.pdf%20http://arxiv.org/abs/1701.07875)

**[13]** One cycle (Just notes)
- Smith, L. N. (2017, March). Cyclical learning rates for training neural networks. In 2017 IEEE Winter Conference on Applications of Computer Vision (WACV) (pp. 464-472). IEEE.
[Paper](https://arxiv.org/pdf/1506.01186.pdf%EF%BC%89%EF%BC%8C%E8%BF%99%E7%A7%8D%E5%A5%87%E6%8A%80%E6%B7%AB%E5%B7%A7%E5%B0%86%E8%8E%B7%E5%BE%97%E6%9B%B4%E9%AB%98%E7%9A%84%E6%B5%8B%E8%AF%95%E5%87%86%E7%A1%AE%E7%8E%87%EF%BC%8C%E4%BD%86%E6%98%AF%E4%BD%A0%E7%9C%8B%E8%BF%99%E4%B8%AAlearning)

**[14]** A disciplined approach to neural network hyper-parameters (Just notes)
- Smith, L. N. (2018). A disciplined approach to neural network hyper-parameters: Part 1--learning rate, batch size, momentum, and weight decay. arXiv preprint arXiv:1803.09820.
[Paper](https://arxiv.org/pdf/1803.09820)

**[15]** Class Imbalance Problem (Just notes)
- Buda, M., Maki, A., & Mazurowski, M. A. (2018). A systematic study of the class imbalance problem in convolutional neural networks. Neural Networks, 106, 249-259. [Paper](https://arxiv.org/pdf/1710.05381)

**[16]** Perceptual Loss (For super resolution)
- Johnson, J., Alahi, A., & Fei-Fei, L. (2016, October). Perceptual losses for real-time style transfer and super-resolution. In European conference on computer vision (pp. 694-711). Springer, Cham. [Paper](https://arxiv.org/pdf/1603.08155.pdf%7C)

**[17]** Semantic segmentation DeepLab
- Chen, L. C., Papandreou, G., Kokkinos, I., Murphy, K., & Yuille, A. L. (2017). Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4), 834-848. [Paper](https://arxiv.org/pdf/1606.00915)

**[18]** Neural Fabrics(WIP)
- Saxena, S., & Verbeek, J. (2016). Convolutional neural fabrics. In Advances in Neural Information Processing Systems (pp. 4053-4061).

**[19]** Focal Loss
- Lin, T. Y., Goyal, P., Girshick, R., He, K., & Dollár, P. (2017). Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision (pp. 2980-2988). [Paper](http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf)

**[20]** Thinking machines - Turing (Just notes)
- A. M. TURING, I.—COMPUTING MACHINERY AND INTELLIGENCE, Mind, Volume LIX, Issue 236, October 1950, Pages 433–460. [Paper](https://oup.silverchair-cdn.com/UI/app/svg/pdf.svg)

**[21]** Unets
- Ronneberger, O., Fischer, P., & Brox, T. (2015, October). U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention (pp. 234-241). Springer, Cham. [Paper](https://arxiv.org/pdf/1505.04597.pdf)

**[22]** LSTM
- Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780.[Paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf)

**[23]** SELU
- Klambauer, G., Unterthiner, T., Mayr, A., & Hochreiter, S. (2017). Self-normalizing neural networks. In Advances in neural information processing systems (pp. 971-980). [Paper](http://papers.nips.cc/paper/6698-self-normalizing-neural-networks.pdf)

**[24]** swish
- Ramachandran, P., Zoph, B., & Le, Q. V. (2017). Searching for activation functions. arXiv preprint arXiv:1710.05941. [Paper](https://arxiv.org/pdf/1710.05941;%20http://arxiv.org/abs/1710.05941)

**[24]** Bag Of Tricks
- He, T., Zhang, Z., Zhang, H., Zhang, Z., Xie, J., & Li, M. (2019). Bag of tricks for image classification with convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 558-567). [Paper](http://openaccess.thecvf.com/content_CVPR_2019/papers/He_Bag_of_Tricks_for_Image_Classification_with_Convolutional_Neural_Networks_CVPR_2019_paper.pdf)

**[25]** Vanilla GNN (WIP)
- Kipf, T. N., & Welling, M. (2016). Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907. [Paper]https://arxiv.org/pdf/1609.02907.pdf?source=post_page---------------------------)

**[26]** Singing voice separation with deep U-Net (WIP)
- Jansson, A., Humphrey, E., Montecchio, N., Bittner, R., Kumar, A., & Weyde, T. (2017). Singing voice separation with deep U-Net convolutional networks. [Paper](https://openaccess.city.ac.uk/id/eprint/19289/1/)

**[27]** HRNet (WIP)
- Wang, J., Sun, K., Cheng, T., Jiang, B., Deng, C., Zhao, Y., ... & Liu, W. (2020). Deep high-resolution representation learning for visual recognition. IEEE transactions on pattern analysis and machine intelligence. [Paper](https://arxiv.org/pdf/1908.07919)

**[28]** Understanding Deep learning requires rethinking generalization) (Just notes
- Zhang, C., Bengio, S., Hardt, M., Recht, B., & Vinyals, O. (2016). Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530. [Paper](https://arxiv.org/pdf/1611.03530.pdf?from=timeline&isappinstalled=0)

**[29]** NVAE (WIP)
- Vahdat, A., & Kautz, J. (2020). NVAE: A Deep Hierarchical Variational Autoencoder. arXiv preprint arXiv:2007.03898. [Paper](https://arxiv.org/abs/2007.03898)

**[30]** DRL and neuroscience (Just notes)
- Botvinick, M., Wang, J. X., Dabney, W., Miller, K. J., & Kurth-Nelson, Z. (2020). Deep Reinforcement Learning and Its Neuroscientific Implications. Neuron. [Paper](https://arxiv.org/pdf/2007.03750)

**[31]** Computational Limits (Just notes)
- Thompson, N. C., Greenewald, K., Lee, K., & Manso, G. F. (2020). The Computational Limits of Deep Learning. arXiv preprint arXiv:2007.05558. [Paper](https://arxiv.org/pdf/2007.05558)

**[32]** What is the state of pruning
- Blalock, D., Ortiz, J. J. G., Frankle, J., & Guttag, J. (2020). What is the state of neural network pruning?. arXiv preprint arXiv:2003.03033. [Paper](https://arxiv.org/pdf/2003.03033.pdf)

**[33]** Super Resolution using Sub Pixel Convolutions
- Shi, W., Caballero, J., Huszár, F., Totz, J., Aitken, A. P., Bishop, R., ... & Wang, Z. (2016). Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1874-1883).  [Paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Shi_Real-Time_Single_Image_CVPR_2016_paper.pdf)

**[34]** Google Keyboard Federated Learning(Notes only. refer to [35] for code)
- Chen, M., Mathews, R., Ouyang, T., & Beaufays, F. (2019). Federated learning of out-of-vocabulary words. arXiv preprint arXiv:1903.10635.  [Paper](https://arxiv.org/pdf/1903.10635)

**[35]** Federated Learning (original paper)
- Konečný, J., McMahan, H. B., Yu, F. X., Richtárik, P., Suresh, A. T., & Bacon, D. (2016). Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492.  [Paper](https://arxiv.org/pdf/1610.05492)
